# Custom section is used to store configurations that might be repetative.
# Please read YAML documentation for details on how to use substitutions and anchors.
custom:
  basic-cluster-props: &basic-cluster-props
    spark_version: "11.3.x-cpu-ml-scala2.12"

  basic-static-cluster: &basic-static-cluster
    new_cluster:
      <<: *basic-cluster-props
      num_workers: 0
      node_type_id: "i3.xlarge"
      spark_conf:
        spark.master: "local[*, 4]"
        spark.databricks.cluster.profile: "singleNode"
      custom_tags:
        ResourceClass: "SingleNode"


environments:
  default:
    workflows:
      #######################################################################################
      #   Populate Feature Tables                                                           #
      #######################################################################################
      - name: "ml-prod-deployment-create-features"
        git_source:
          git_url: https://github.com/marshackVB/ml_prod_deployment.git
          git_provider: "github"
          git_branch: "master"
        schedule: 
          quartz_cron_expression: "35 0 0 * * ?"
          timezone_id: "UTC"
          pause_status": "UNPAUSED"
        tasks:
          - task_key: 'create-features'
            <<: *basic-static-cluster
            notebook_task:
              notebook_path: 'ml_prod_deployment/create_features'
            deployment_config:
              no_package: true
      #######################################################################################
      #   Train model                                                                       #
      #######################################################################################
      - name: "ml-prod-deployment-train-model"
        git_source:
          git_url: https://github.com/marshackVB/ml_prod_deployment.git
          git_provider: "github"
          git_branch: "master"
        tasks:
          - task_key: 'train-model'
            <<: *basic-static-cluster
            notebook_task:
              notebook_path: 'ml_prod_deployment/trainer'
            deployment_config:
              no_package: true
      #######################################################################################
      #   Example workflow for integration tests                                            #
      #######################################################################################
      - name: "ml-prod-deployment-sample-tests"
        tasks:
          - task_key: "main"
            <<: *basic-static-cluster
            spark_python_task:
                python_file: "file://tests/entrypoint.py"
                # this call supports all standard pytest arguments
                parameters: ["file:fuse://tests/integration", "--cov=ml_prod_deployment"]
      #######################################################################################
      # this is an example job with single ETL task based on 2.1 API and wheel_task format #
      ######################################################################################
      - name: "ml-prod-deployment-sample-etl"
        tasks:
          - task_key: "main"
            <<: *basic-static-cluster
            python_wheel_task:
              package_name: "ml_prod_deployment"
              entry_point: "etl" # take a look at the setup.py entry_points section for details on how to define an entrypoint
              parameters: ["--conf-file", "file:fuse://conf/tasks/sample_etl_config.yml"]
      #############################################################
      # this is an example multitask job with notebook task       #
      #############################################################
      - name: "ml-prod-deployment-sample-multitask"
        job_clusters:
          - job_cluster_key: "default"
            <<: *basic-static-cluster
        tasks:
          - task_key: "etl"
            job_cluster_key: "default"
            spark_python_task:
              python_file: "file://ml_prod_deployment/tasks/sample_etl_task.py"
              parameters: [ "--conf-file", "file:fuse://conf/tasks/sample_etl_config.yml" ]
          - task_key: "ml"
            depends_on:
              - task_key: "etl"
            job_cluster_key: "default"
            python_wheel_task:
              package_name: "ml_prod_deployment"
              entry_point: "ml"
              parameters: [ "--conf-file", "file:fuse://conf/tasks/sample_ml_config.yml" ]
          ###############################################################################
          # this is an example task based on the notebook                               #
          # Please note that first you'll need to add a Repo and commit notebook to it. #
          ###############################################################################
          - task_key: "notebook"
            deployment_config:
              no_package: true # we omit using package since code will be shipped directly from the Repo
            depends_on:
              - task_key: "ml"
            job_cluster_key: "default"
            notebook_task:
              notebook_path: "/Repos/Staging/ml_prod_deployment/notebooks/sample_notebook"